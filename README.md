# SIESTA Preprocess Component
This module is responsible to create the indices that are then utilized by the query processor
to respond fast in pattern queries. Along with the generation of the SIESTA indices, we have implemented
the preprocess step for both Signature and Set-Containment methods.

This module is responsible to create the indices that will be then utilized by the 
[query processor](https://github.com/mavroudo/SequenceDetectionQueryExecutor)to respond 
fast in pattern queries. The primary inverted index (named IndexTable) along with the other auxiliary tables
are generated from the provided logfile combined with any previous already stored indices, utilizing Apache Spark,
a framework designed for efficiently handling big data. Along with the generation of the SIESTA indices, we
implemented two additional indexing techniques, namely Signature and Set-Containment to evaluate their
performance. Our complete work can be found [here](https://ieeexplore.ieee.org/document/9984935). 



## Execute program using docker
1. In case a database is already deployed (either S3 or Cassandra) pass the configure parameters at the end of the Dockerfile.
If that is not the case and a local instance of this databases needs to be deployed use the following commands:

- ```docker-compose -f dockerbase/docker-compose-s3.yml up```  for the S3
- ```docker-compose -f dockerbase/docker-compose-cassabdra.yml up```  for the Cassandra

2. If a spark cluster is already running, its url can be defined in the ENTRYPOINT of the Dockerfile.
 Otherwise, it will deploy and use a local instance of Spark, using all the available cores.
3. **Build docker image:** On the top level of the project execute the command ```docker build -t preprocess .```
4. **Run image**: After image was built it can be run with ```docker run preprocess```
The program provides a range of parameters that can be set as arguments during execution. 
The default execution will generate 200 synthetic traces, using 10 different event types, and lengths that vary from 10 to 90 events.
It is a good way to evaluate that the build was successful.

Complete list of parameters:
```
  --system <system>        System refers to the system that will be used for indexing
  -d, --database <database>
                           Database refers to the database that will be used to store the index
  -m, --mode <mode>        Mode will determine if we use the timestamps or positions in indexing
  -c, --compression <compression>
                           Compression determined the algorithm that will be used to compress the indexes
  -f, --file <file>        If not set will generate artificially data
  --logname <logname>      Specify the name of the index to be created. This is used in case of incremental preprocessing
  --delete_all             cleans all tables in the keyspace
  --delete_prev            removes all the tables generated by a previous execution of this method
  --join                   merges the traces with the already indexed ones
  --lookback <value>       How many days will look back for completions (default=30)
  -s, --split_every_days s
                           Split the inverted index every s days (default=30)

The parameters below are used if the file was not set and data will be randomly generated
  -t, --traces <#traces>
  -e, --event_types <#event_types>
  --lmin <min length>
  --lmax <max length>
  --help                   prints this usage text

```
Additional Notes:
* The master url was set it in the ENTRYPOINT of the Dockerfile, along with some configuration parameters.
Any master can be used like yarn, or spark cluster e.t.c. Moreover, cassandra and sparks parameters can also
be changed in order to match the studied scenario.
* In order to use xes files, they need to pass through volume inside the docker container.
The docker run command will be look something like 
```
docker run \
  --mount type=bind,source="$(pwd)"/experiments/input,target=/app/input \
  preprocess -f /input/dataset.xes
```